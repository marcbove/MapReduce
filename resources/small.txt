MapReduce is a programming model and implementation to enable the
parallel processing of huge amounts of data. In a nutshell, it breaks
a large dataset into smaller chunks to be processed separately on
different worker nodes and automatically gathers the results across
the multiple nodes to return a single result. 

As it name suggests, it allows for distributed processing of the map()
and reduce() functional operations, which carry out most of the
programming logic. Indeed, it consists of three majors steps, which
are the following ones: 

"Map" step: Each worker node applies the "map()" function to the local
data, and writes the output to a temporary storage. A master node
ensures that only one copy of redundant input data is processed.
"Shuffle" step: Worker nodes redistribute data based on the output
keys (produced by the "map()" function), such that all data belonging
to one key is located on the same worker node.
"Reduce" step: Worker nodes now process each group of output data, per
key, in parallel.

Because each mapping operation is independent of the others, all maps
can be performed in parallel. The same occurs to the reducers, given
that all outputs of the map operation with the same key are handed to
the same reducer.
